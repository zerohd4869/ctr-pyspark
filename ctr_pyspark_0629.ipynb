{"cells":[{"metadata":{"id":"69C9A5EBE20847FE8BD0253C19FBB1CA","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"! killall python","execution_count":57},{"outputs":[],"execution_count":1,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/\n# !ps aux","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"E76783B6283945B5BD519131880485F7","scrolled":false}},{"outputs":[{"output_type":"stream","text":"1kw_train_data.csv  models\t       result_0629_1.csv     test_data.csv\r\nkesci_submit\t    result_0627_1.csv  result_0629_demo.csv\r\nlost+found\t    result_0627_2.csv  spark-warehouse\r\n","name":"stdout"}],"execution_count":2,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"56A3614E19024158934D1C7CB5C33A1A","scrolled":false}},{"outputs":[],"execution_count":3,"source":"# 查看当前kernerl下的package\n# !pip list --format=columns","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"756A75B0B8764A5984F53F02FBC4D909","scrolled":false}},{"outputs":[],"execution_count":4,"source":"# 显示cell运行时长\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"F66795DC50134FDE9EA2205A9243DF67","scrolled":false}},{"metadata":{"id":"E1BCEF052F7243318CC38487273E71F8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"/home/kesci/work\ntotal 9.0G\n-rw-r--r-- 1 kesci 1000 426M May 22 04:05 test.csv\n-rw-r--r-- 1 kesci 1000 8.6G May 16 20:15 train.csv\n-rw-r--r-- 1 kesci users 946M Jun 20 10:25 ./1kw_train_data.csv\n-rw-r--r-- 1 kesci users 463M Jun 24 15:28 ./test_data.csv\ntime: 1.92 s\n","name":"stdout"}],"source":"#数据查看路径如下\ntrain_path='/home/kesci/input/bytedance/first-round/train.csv'\nraw_test_path='/home/kesci/input/bytedance/first-round/test.csv'\nwork_path='/home/kesci/work/'\ntrain_path_1kw = './1kw_train_data.csv'\ntest_path = './test_data.csv'\n\n!pwd\n!ls -lh /home/kesci/input/bytedance/first-round/\n!ls -lh $train_path_1kw\n!ls -lh $test_path","execution_count":5},{"metadata":{"id":"1CFC536AD04342E48D51B35513B5223B","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 717 ms\n","name":"stdout"}],"source":"import pandas as pd\nimport numpy as np\nfrom math import *\n\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, concat, concat_ws, col, corr, avg\nimport pyspark.sql.functions as F","execution_count":6},{"metadata":{"id":"C6C7CC34B94C4EF88C08A280FF42E407","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 642 µs\n","name":"stdout"}],"source":"# # 运行一次  转换1kw数据  3min\n# with open(train_path, 'r') as f:\n#     # header_names=['query_id','query','title','label']\n#     csv_reader = pd.read_csv(f, encoding=\"utf-8\", engine='python',header=None,nrows=10000000)\n#     csv_reader.to_csv(train_path_1kw,index=1)\n\n\n# # with open(raw_test_path, 'r') as f:\n# #     # header_names=['query_id','query','title','label']\n# #     csv_reader = pd.read_csv(f, encoding=\"utf-8\", engine='python',header=None)\n# #     csv_reader.to_csv(test_path,index=1)\n# header_names=['index','query_id','query','title','label']\n# pd.read_csv(open(test_path,'r'), encoding=\"utf-8\", engine='python',nrows=5)    ","execution_count":7},{"metadata":{"id":"D5671088324B4CB3ABC45E4B81FB5981","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":11},{"metadata":{"id":"74D800C33E4D4EA1892E3EBD8063023F","collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"[('index', 'int'), ('query_id', 'int'), ('query', 'string'), ('query_title_id', 'int'), ('title', 'string'), ('label', 'int'), ('text', 'string')] 5000000\ntime: 22.1 s\n","name":"stdout"}],"source":"# all train data  \nspark = SparkSession.builder.appName('ctr_pyspark_0609') \\\n    .config(\"spark.driver.memory\", \"29g\") \\\n    .config(\"spark.executor.memory\", \"29g\") \\\n    .config(\"spark.driver.maxResultSize\",\"29g\") \\\n    .config(\"spark.dynamicAllocation.enabled\",\"true\") \\\n    .config(\"spark.logConf\", \"true\") \\\n    .getOrCreate()\n\n    \nsdf_all = spark.read.csv(train_path_1kw, header=False) \\\n    .toDF(\"index\", \"query_id\", \"query\", \"query_title_id\", \"title\", 'label')\nsdf_all = sdf_all.filter(sdf_all.index >= 5000000) \\\n    .withColumn(\"index\", sdf_all.index.cast(\"Int\")) \\\n    .withColumn(\"label\", sdf_all.label.cast(\"Int\")) \\\n    .withColumn(\"query_id\", sdf_all.query_id.cast(\"Int\")) \\\n    .withColumn(\"query_title_id\", sdf_all.query_title_id.cast(\"Int\")) \\\n    .withColumn(\"text\", concat_ws(' ', sdf_all.query, sdf_all.title))\n# \nall_count = sdf_all.count()\nprint(sdf_all.dtypes,all_count)\n# sdf_all.show(2,truncate=True)","execution_count":8},{"metadata":{"id":"19C8510746CB4FE8893AFFB88FE80271","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 564 µs\n","name":"stdout"}],"source":"# !spark-shell  --help","execution_count":9},{"metadata":{"id":"AC3146D30C01457E8ED2B0AE86682CDA","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"071BC4BCB6F74CFC9DBADAF56ED2E906","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"[('index', 'int'), ('query_id', 'int'), ('query', 'string'), ('query_title_id', 'int'), ('title', 'string'), ('text', 'string')]\ntime: 632 ms\n","name":"stdout"}],"source":"# test data\nsdf_test = spark.read.csv(test_path,header=True) \\\n    .toDF(\"index\",\"query_id\", \"query\", \"query_title_id\", \"title\")\nsdf_test = sdf_test.withColumn(\"index\", sdf_test.index.cast(\"Int\")) \\\n    .withColumn(\"text\", concat_ws(' ',sdf_test.query,sdf_test.title)) \\\n    .withColumn(\"query_id\", sdf_test.query_id.cast(\"Int\")) \\\n    .withColumn(\"query_title_id\", sdf_test.query_title_id.cast(\"Int\")) \\\n    # .filter(sdf_test.index < 100)\n\nprint(sdf_test.dtypes)\n# sdf_test.show(2,truncate=True)","execution_count":10},{"metadata":{"id":"9B591858312D4503BF9B235B03AFF76E","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 433 µs\n","name":"stdout"}],"source":"# reset datasets\n# sdf_all = sdf_all.select(\"index\", \"query_id\", \"query\", \"query_title_id\", \"title\",\"text\",\"label\")\n# sdf_test = sdf_test.select(\"index\",\"query_id\", \"query\", \"query_title_id\", \"title\",\"text\")","execution_count":11},{"metadata":{"id":"C5947A4624D44CC49D2F4DA926B7F26F","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"[('index', 'int'), ('query_id', 'int'), ('query', 'string'), ('query_title_id', 'int'), ('title', 'string'), ('label', 'int'), ('text', 'string'), ('texts', 'array<string>'), ('queries', 'array<string>'), ('titles', 'array<string>')]\n[('index', 'int'), ('query_id', 'int'), ('query', 'string'), ('query_title_id', 'int'), ('title', 'string'), ('text', 'string'), ('texts', 'array<string>'), ('queries', 'array<string>'), ('titles', 'array<string>')]\ntime: 299 ms\n","name":"stdout"}],"source":"from pyspark.ml.feature import Normalizer,MinHashLSH,HashingTF,IDF,IDFModel,Tokenizer,Word2Vec, Word2VecModel,IndexToString, StringIndexer, VectorIndexer\n\n# tokenizer \ntokenizer = Tokenizer(inputCol='text', outputCol='texts')\nsdf_all = tokenizer.transform(sdf_all)\nsdf_all = tokenizer.transform(sdf_all,params={tokenizer.inputCol:\"query\",tokenizer.outputCol:\"queries\"})\nsdf_all = tokenizer.transform(sdf_all,params={tokenizer.inputCol:\"title\",tokenizer.outputCol:\"titles\"})\nsdf_test = tokenizer.transform(sdf_test)\nsdf_test = tokenizer.transform(sdf_test,params={tokenizer.inputCol:\"query\",tokenizer.outputCol:\"queries\"})\nsdf_test = tokenizer.transform(sdf_test,params={tokenizer.inputCol:\"title\",tokenizer.outputCol:\"titles\"})\n\nprint(sdf_all.dtypes)\nprint(sdf_test.dtypes)\n","execution_count":12},{"metadata":{"id":"29E8CB0B13464E8AB70488916D37DBB2","collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"code","outputs":[],"source":"# word-vector\n\nword2vecModelPath =  \"./models/word2vec-model-1kw-50-1-2\" \nidfModelPath =  \"./models/idf-model-1kw-100000-2\" \n\n# word2vec\n# word2vec = Word2Vec(inputCol=\"texts\",outputCol='word2vec',numPartitions=64,vectorSize=50,maxIter=1,minCount=2,seed=2019)\n# print(\"word2vec parameters:\\n\" + word2vec.explainParams())\n# word2vecModel = word2vec.fit(sdf_test.select(\"texts\").unionByName(sdf_all.select(\"texts\")))\n# word2vecModel.save(word2vecModelPath)\n\n# idf\n# tf = HashingTF(inputCol=\"texts\", outputCol=\"tf\", numFeatures=100000)\n# idf = IDF(inputCol=\"tf\", outputCol=\"idf\",minDocFreq=2)\n# print(\"tf parameters:\\n\" + tf.explainParams())\n# print(\"idf parameters:\\n\" + idf.explainParams())\n# idfModel = idf.fit(tf.transform(sdf_test).select(\"tf\").unionByName(tf.transform(sdf_all).select(\"tf\")))\n# idfModel.save(idfModelPath)\n\n# idfModel.save(idfModelPath)","execution_count":13},{"metadata":{"id":"D40A32B7B74A43F3932105E5CB91A55C","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":14},{"metadata":{"id":"C0526C3C332D420181E58CF4F4EB5585","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# word2vec\nword2vecModel = Word2VecModel.load(word2vecModelPath)\nsdf_all = word2vecModel.transform(sdf_all,params={word2vecModel.inputCol:\"titles\",word2vecModel.outputCol:\"word2vec_title\"})\nsdf_all = word2vecModel.transform(sdf_all,params={word2vecModel.inputCol:\"queries\",word2vecModel.outputCol:\"word2vec_query\"})\nsdf_test = word2vecModel.transform(sdf_test,params={word2vecModel.inputCol:\"titles\",word2vecModel.outputCol:\"word2vec_title\"})\nsdf_test = word2vecModel.transform(sdf_test,params={word2vecModel.inputCol:\"queries\",word2vecModel.outputCol:\"word2vec_query\"})\n\n# tfidf\ntf = HashingTF(inputCol=\"texts\", outputCol=\"tf\", numFeatures=100000)\nidfModel = IDFModel.load(idfModelPath)\nnormalizer = Normalizer(inputCol=\"idf\", outputCol=\"norm_idf\")\n\nsdf_all = tf.transform(sdf_all,params={tf.inputCol:\"titles\",tf.outputCol:\"tf_title\"})\nsdf_all = tf.transform(sdf_all,params={tf.inputCol:\"queries\",tf.outputCol:\"tf_query\"})\nsdf_test = tf.transform(sdf_test,params={tf.inputCol:\"titles\",tf.outputCol:\"tf_title\"})\nsdf_test = tf.transform(sdf_test,params={tf.inputCol:\"queries\",tf.outputCol:\"tf_query\"})\n\nsdf_all = idfModel.transform(sdf_all,params={idfModel.inputCol:\"tf_title\",idfModel.outputCol:\"idf_title\"})\nsdf_all = idfModel.transform(sdf_all,params={idfModel.inputCol:\"tf_query\",idfModel.outputCol:\"idf_query\"})\nsdf_test = idfModel.transform(sdf_test,params={idfModel.inputCol:\"tf_title\",idfModel.outputCol:\"idf_title\"})\nsdf_test = idfModel.transform(sdf_test,params={idfModel.inputCol:\"tf_query\",idfModel.outputCol:\"idf_query\"})\n\nsdf_all = normalizer.transform(sdf_all,params={normalizer.inputCol:\"idf_title\",normalizer.outputCol:\"norm_idf_title\"})\nsdf_all = normalizer.transform(sdf_all,params={normalizer.inputCol:\"idf_query\",normalizer.outputCol:\"norm_idf_query\"})\nsdf_test = normalizer.transform(sdf_test,params={normalizer.inputCol:\"idf_title\",normalizer.outputCol:\"norm_idf_title\"})\nsdf_test = normalizer.transform(sdf_test,params={normalizer.inputCol:\"idf_query\",normalizer.outputCol:\"norm_idf_query\"})\n\n","execution_count":14},{"metadata":{"id":"8A5F8C5421804A5A85113B09C9F72D2C","collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"code","source":"sdf_all.dtypes","execution_count":17,"outputs":[]},{"metadata":{"id":"ABE70F718AA74CFE9650837EB302989C","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"from pyspark.ml.feature import NGram\n\nbiGram = NGram(n=2, inputCol=\"texts\", outputCol=\"bi_texts\") # ngram.setParams(n=4)\n\nsdf_all = biGram.transform(sdf_all)\nsdf_all = biGram.transform(sdf_all,params={biGram.inputCol:\"queries\",biGram.outputCol:\"bi_queries\"})\nsdf_all = biGram.transform(sdf_all,params={biGram.inputCol:\"titles\",biGram.outputCol:\"bi_titles\"})\nsdf_test = biGram.transform(sdf_test)\nsdf_test = biGram.transform(sdf_test,params={biGram.inputCol:\"queries\",biGram.outputCol:\"bi_queries\"})\nsdf_test = biGram.transform(sdf_test,params={biGram.inputCol:\"titles\",biGram.outputCol:\"bi_titles\"})\n\n#bi\n\nbiWord2vecModelPath =  \"./models/bi-word2vec-model-1kw-50-1-2\" \nbiIdfModelPath =  \"./models/bi-idf-model-1kw-100000-2\" \n# word2vec\nword2vec = Word2Vec(inputCol=\"bi_texts\",outputCol='bi_word2vec',numPartitions=64,vectorSize=50,maxIter=1,minCount=2,seed=2019)\nword2vecModel = word2vec.fit(sdf_test.select(\"bi_texts\").unionByName(sdf_all.select(\"bi_texts\")))\nword2vecModel.save(biWord2vecModelPath)\n\n# # idf\ntf = HashingTF(inputCol=\"bi_texts\", outputCol=\"bi_tf\", numFeatures=100000)\nidf = IDF(inputCol=\"bi_tf\", outputCol=\"bi_idf\",minDocFreq=2)\nidfModel = idf.fit(tf.transform(sdf_test).select(\"bi_tf\").unionByName(tf.transform(sdf_all).select(\"bi_tf\")))\nidfModel.save(biIdfModelPath)\n","execution_count":16},{"metadata":{"id":"1930206F61C04C8B8BD7A9900B34FCCF","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# bi\n# word2vec\nbiWord2vecModel = Word2VecModel.load(biWord2vecModelPath)\nsdf_all = biWord2vecModel.transform(sdf_all,params={biWord2vecModel.inputCol:\"bi_titles\",biWord2vecModel.outputCol:\"bi_word2vec_title\"})\nsdf_all = biWord2vecModel.transform(sdf_all,params={biWord2vecModel.inputCol:\"bi_queries\",biWord2vecModel.outputCol:\"bi_word2vec_query\"})\nsdf_test = biWord2vecModel.transform(sdf_test,params={biWord2vecModel.inputCol:\"bi_titles\",biWord2vecModel.outputCol:\"bi_word2vec_title\"})\nsdf_test = biWord2vecModel.transform(sdf_test,params={biWord2vecModel.inputCol:\"bi_queries\",biWord2vecModel.outputCol:\"bi_word2vec_query\"})\n\n# tfidf\nbiTf = HashingTF(inputCol=\"bi_texts\", outputCol=\"bi_tf\", numFeatures=10)\nbiIdfModel = IDFModel.load(biIdfModelPath)\nbiNormalizer = Normalizer(inputCol=\"bi_idf\", outputCol=\"bi_norm_idf\")\n\nsdf_all = biTf.transform(sdf_all,params={biTf.inputCol:\"bi_titles\",biTf.outputCol:\"bi_tf_title\"})\nsdf_all = biTf.transform(sdf_all,params={biTf.inputCol:\"bi_queries\",biTf.outputCol:\"bi_tf_query\"})\nsdf_test = biTf.transform(sdf_test,params={biTf.inputCol:\"bi_titles\",biTf.outputCol:\"bi_tf_title\"})\nsdf_test = biTf.transform(sdf_test,params={biTf.inputCol:\"bi_queries\",biTf.outputCol:\"bi_tf_query\"})\n\nsdf_all = biIdfModel.transform(sdf_all,params={biIdfModel.inputCol:\"bi_tf_title\",biIdfModel.outputCol:\"bi_idf_title\"})\nsdf_all = biIdfModel.transform(sdf_all,params={biIdfModel.inputCol:\"bi_tf_query\",biIdfModel.outputCol:\"bi_idf_query\"})\nsdf_test = biIdfModel.transform(sdf_test,params={biIdfModel.inputCol:\"bi_tf_title\",biIdfModel.outputCol:\"bi_idf_title\"})\nsdf_test = biIdfModel.transform(sdf_test,params={biIdfModel.inputCol:\"bi_tf_query\",biIdfModel.outputCol:\"bi_idf_query\"})\n\nsdf_all = biNormalizer.transform(sdf_all,params={biNormalizer.inputCol:\"bi_idf_title\",biNormalizer.outputCol:\"bi_norm_idf_title\"})\nsdf_all = biNormalizer.transform(sdf_all,params={biNormalizer.inputCol:\"bi_idf_query\",biNormalizer.outputCol:\"bi_norm_idf_query\"})\nsdf_test = biNormalizer.transform(sdf_test,params={biNormalizer.inputCol:\"bi_idf_title\",biNormalizer.outputCol:\"bi_norm_idf_title\"})\nsdf_test = biNormalizer.transform(sdf_test,params={biNormalizer.inputCol:\"bi_idf_query\",biNormalizer.outputCol:\"bi_norm_idf_query\"})\n\n","execution_count":17},{"metadata":{"id":"3E52F3C282904F498CD92B87AA21F5DF","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"sdf_all.dtypes\n# sdf_all.select('title').groupBy('title').count().filter(col('count')>50).orderBy(col(\"count\").desc()).collect()","execution_count":18},{"metadata":{"id":"54E5785625EE43758CA1449894593106","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":26},{"metadata":{"id":"DC05F27650544177BB21D2D1FD5C64A5","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import numpy as np\nimport time\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nimport Levenshtein\n\ndef dist(A,B,mode):\n        \n    if mode == \"euclidean\":\n        sim = np.linalg.norm(A - B)  # euclidean\n    elif mode == \"cosine\":\n        num = np.sum(np.multiply(A,B))\n        denom = np.linalg.norm(A) * np.linalg.norm(B)\n        if denom == 0:\n            denom = 1e-6   # 除0处理\n        sim = np.divide(num, denom)  # 0.5 + 0.5 * cos\n    elif  mode == \"edit\":\n        sim  = Levenshtein.distance(A,B)\n    return round(float(sim),6)\n  \ndef square_rooted(x):\n    return sqrt(sum([a*a for a in x]))\n    \ndef cosine_sim(x,y):\n    x,y = [float(i) for i in x],[float(j) for j in y]\n    numerator = sum(a*b for a,b in zip(x,y))\n    denominator = square_rooted(x)*square_rooted(y)\n    return round(float(np.divide(numerator,denominator+1e-6)),6)\n\ndef jaccard_sim(x,y):\n    x, y = set(x), set(y)\n    inter = len(set.intersection(*[x, y]))\n    union = len(set.union(*[x, y]))\n    return round(float(np.divide(inter,union)),6)\n\ndef sim(x, y, mode=\"cosine\"):\n    if mode == \"jaccard\":\n        d = jaccard_sim(x, y)\n    elif mode == \"cosine\":\n        d = cosine_sim(x, y)\n    return round(float(d),6)\n\ndef title_len(x):\n    return len(x[\"titles\"])\n\ndef word_count(x):\n    return sum(1 for i in x['titles'] if i in x['queries'])\n\n\ndef get_pos(query, text):\n    return [round(float( np.divide(len(text),(1+text.index(i))) ),6)  if i in set(text) else 0. for i in set(query)]\n    \ndef mean_pos(x):\n    return round(float(np.mean(get_pos(x['queries'], x['titles']))),6)\n\ndef word_ratio(x):\n    return round(float(np.divide(len(set(x['queries'])),len(set(x['titles'])))),6)\n\ndef get_qt(query, title):\n    # query在title中的位置  平均值\n    return [title.index(i)  if i in set(title) else len(title) for i in set(query)]\ndef get_qt_min(query, title):\n    # query在text中的位置  max\n    tmp = get_qt(query, title)\n    return int(tmp[np.argmin(tmp)])\ndef get_qt_mean(query, title):\n    # query在text中的位置  max\n    tmp = get_qt(query, title)\n    return round(float(np.mean(tmp)),6)\n# \ndef sum_hit(x,y):\n    # print('sum_hit')\n    return sum(x.count(i) for i in set(y))\ndef mean_hit(x,y):\n    # print('mean_hit')\n    return  round(float(np.mean([x.count(i) for i in set(y)])),6)\ndef max_hit(x,y):\n    # print('max_hit')\n    return max(x.count(i) for i in set(y) )\n    ","execution_count":19},{"metadata":{"id":"8E6209215792453F9483729E92E33AEE","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"row = Row(\"index\",\"w2v_eu_dist\",\"w2v_cos_dist\",\"edit_dist\",\n    \"text_cos_sim\",\"text_jaccard_sim\",\"idf_cos_sim\",\n    \"title_len\",'word_count','mean_pos','word_ratio',\n    \"qt_min\",\"qt_mean\",\"tq_min\",\"tq_mean\",\n    \"sum_hit\",\"mean_hit\",\"max_hit\",\n    \"bi_1\",\"bi_2\",\"bi_3\",\"bi_4\",\"bi_5\",\"bi_6\",\"bi_7\",\"bi_8\",\"bi_9\",\"bi_10\",\"bi_11\",\"bi_12\",\"bi_13\",\"bi_14\",\"bi_15\")    \n#  title_len \nfea_all = sdf_all.select(\n        \"index\",\"title\",\"query\",\"titles\",\"queries\",\"word2vec_title\",\"word2vec_query\",\"norm_idf_title\",\"norm_idf_query\", \\\n         \"bi_titles\",\"bi_queries\",\"bi_word2vec_title\",\"bi_word2vec_query\",\"bi_norm_idf_title\",\"bi_norm_idf_query\"\n    ).rdd.map(lambda x:row(\n            x[\"index\"],\n            dist(x[\"word2vec_title\"],x['word2vec_query'],mode=\"euclidean\"), \n            dist(x[\"word2vec_title\"],x['word2vec_query'],mode=\"cosine\"),\n            dist(x['title'], x['query'],mode=\"edit\"),\n            sim(x['titles'], x['queries'],mode=\"cosine\"),\n            sim(x['titles'], x['queries'],mode=\"jaccard\"),\n            sim(x['norm_idf_title'].toArray(), x['norm_idf_query'].toArray(),mode=\"cosine\"),\n            title_len(x),\n            word_count(x),\n            mean_pos(x),\n            word_ratio(x),\n            get_qt_min(x[\"queries\"],x[\"titles\"]),\n            get_qt_mean(x[\"queries\"],x[\"titles\"]),\n            get_qt_min(x[\"titles\"],x[\"queries\"]),\n            get_qt_mean(x[\"titles\"],x[\"queries\"]),\n            sum_hit(x['titles'],x['queries']),\n            mean_hit(x['titles'],x['queries']),\n            max_hit(x['titles'],x['queries']), \\\n            dist(x[\"bi_word2vec_title\"],x['bi_word2vec_query'],mode=\"euclidean\"), \n            dist(x[\"bi_word2vec_title\"],x['bi_word2vec_query'],mode=\"cosine\"),\n            sim(x['bi_titles'], x['queries'],mode=\"jaccard\"),\n            sim(x['bi_norm_idf_title'].toArray(), x['bi_norm_idf_query'].toArray(),mode=\"cosine\"),\n            len(x[\"bi_titles\"]),\n            sum(1 for i in x['bi_titles'] if i in x['bi_queries']),\n            round(float(np.mean(get_pos(x['bi_queries'], x['bi_titles']))),6),\n            round(float(np.divide(len(set(x['bi_queries'])),len(set(x['bi_titles'])))),6),\n            get_qt_min(x[\"bi_queries\"],x[\"bi_titles\"]),\n            get_qt_mean(x[\"bi_queries\"],x[\"bi_titles\"]),\n            get_qt_min(x[\"bi_titles\"],x[\"bi_queries\"]),\n            get_qt_mean(x[\"bi_titles\"],x[\"bi_queries\"]),\n            sum_hit(x['titles'],x['queries']),\n            mean_hit(x['titles'],x['queries']),\n            max_hit(x['titles'],x['queries'])\n        )\n    ).toDF().join(sdf_all.select('index','query_id','query_title_id','title','label'), on='index')\n\n# test\nfea_test = sdf_test.select( \n        \"index\",\"title\",\"query\",\"titles\",\"queries\",\"word2vec_title\", \"word2vec_query\",\"norm_idf_title\",\"norm_idf_query\", \\\n         \"bi_titles\",\"bi_queries\",\"bi_word2vec_title\",\"bi_word2vec_query\",\"bi_norm_idf_title\",\"bi_norm_idf_query\"\n    ).rdd.map(lambda x:row(\n            x[\"index\"],\n            dist(x[\"word2vec_title\"],x['word2vec_query'],mode=\"euclidean\"), \n            dist(x[\"word2vec_title\"],x['word2vec_query'],mode=\"cosine\"),\n            dist(x['title'], x['query'],mode=\"edit\"),\n            sim(x['titles'], x['queries'],mode=\"cosine\"),\n            sim(x['titles'], x['queries'],mode=\"jaccard\"),\n            sim(x['norm_idf_title'].toArray(), x['norm_idf_query'].toArray(),mode=\"cosine\"),\n            title_len(x),\n            word_count(x),\n            mean_pos(x),\n            word_ratio(x),\n            get_qt_min(x[\"queries\"],x[\"titles\"]),\n            get_qt_mean(x[\"queries\"],x[\"titles\"]),\n            get_qt_min(x[\"titles\"],x[\"queries\"]),\n            get_qt_mean(x[\"titles\"],x[\"queries\"]),\n            sum_hit(x['titles'],x['queries']),\n            mean_hit(x['titles'],x['queries']),\n            max_hit(x['titles'],x['queries']), \\\n            dist(x[\"bi_word2vec_title\"],x['bi_word2vec_query'],mode=\"euclidean\"), \n            dist(x[\"bi_word2vec_title\"],x['bi_word2vec_query'],mode=\"cosine\"),\n            sim(x['bi_titles'], x['queries'],mode=\"jaccard\"),\n            sim(x['bi_norm_idf_title'].toArray(), x['bi_norm_idf_query'].toArray(),mode=\"cosine\"),\n            len(x[\"bi_titles\"]),\n            sum(1 for i in x['bi_titles'] if i in x['bi_queries']),\n            round(float(np.mean(get_pos(x['bi_queries'], x['bi_titles']))),6),\n            round(float(np.divide(len(set(x['bi_queries'])),len(set(x['bi_titles'])))),6),\n            get_qt_min(x[\"bi_queries\"],x[\"bi_titles\"]),\n            get_qt_mean(x[\"bi_queries\"],x[\"bi_titles\"]),\n            get_qt_min(x[\"bi_titles\"],x[\"bi_queries\"]),\n            get_qt_mean(x[\"bi_titles\"],x[\"bi_queries\"]),\n            sum_hit(x['bi_titles'],x['bi_queries']),\n            mean_hit(x['bi_titles'],x['bi_queries']),\n            max_hit(x['bi_titles'],x['bi_queries'])\n        ) \n    ).toDF().join(sdf_test.select('index','query_id','query_title_id','title') , on='index')\n","execution_count":20},{"metadata":{"id":"A1E948803B284A428159F051124A24A7","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"fea_all.dtypes","execution_count":43},{"metadata":{"id":"287D82E9B7C746A781C1EDE2E32C4901","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":62},{"metadata":{"id":"B240E0EAAE4C4F278E34682F84060250","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"\nfea_all.join(fea_all.groupby('query_id').agg(F.max(fea_all.w2v_eu_dist),F.min(fea_all.w2v_eu_dist),\n                                F.max(fea_all.w2v_cos_dist),F.min(fea_all.w2v_cos_dist),\n                                F.max(fea_all.edit_dist),F.min(fea_all.edit_dist),\n                                F.max(fea_all.text_cos_sim),F.min(fea_all.text_cos_sim),\n                                F.max(fea_all.text_jaccard_sim),F.min(fea_all.text_jaccard_sim),\n                                F.max(fea_all.idf_cos_sim),F.min(fea_all.idf_cos_sim),\n                                F.max(fea_all.title_len),F.min(fea_all.title_len),\n                                F.max(fea_all.word_count),F.min(fea_all.word_count),\n                                F.max(fea_all.mean_pos),F.min(fea_all.mean_pos),\n                                F.max(fea_all.word_ratio),F.min(fea_all.word_ratio),\n                                F.max(fea_all.qt_min),F.min(fea_all.qt_min),\n                                F.max(fea_all.qt_mean),F.min(fea_all.qt_mean),\n                                F.max(fea_all.tq_min),F.min(fea_all.tq_min),\n                                F.max(fea_all.tq_mean),F.min(fea_all.tq_mean),\n                                F.max(fea_all.sum_hit),F.min(fea_all.sum_hit),\n                                F.max(fea_all.mean_hit),F.min(fea_all.mean_hit),\n                                F.max(fea_all.max_hit),F.min(fea_all.max_hit),\n                                F.max(fea_all.bi_1),F.min(fea_all.bi_1),\n                                F.max(fea_all.bi_2),F.min(fea_all.bi_2),\n                                F.max(fea_all.bi_3),F.min(fea_all.bi_3),\n                                F.max(fea_all.bi_4),F.min(fea_all.bi_4),\n                                F.max(fea_all.bi_5),F.min(fea_all.bi_5),\n                                F.max(fea_all.bi_6),F.min(fea_all.bi_6),\n                                F.max(fea_all.bi_7),F.min(fea_all.bi_7),\n                                F.max(fea_all.bi_8),F.min(fea_all.bi_8),\n                                F.max(fea_all.bi_9),F.min(fea_all.bi_9),\n                                F.max(fea_all.bi_10),F.min(fea_all.bi_10),\n                                F.max(fea_all.bi_11),F.min(fea_all.bi_11),\n                                F.max(fea_all.bi_12),F.min(fea_all.bi_12),\n                                F.max(fea_all.bi_13),F.min(fea_all.bi_13),\n                                F.max(fea_all.bi_14),F.min(fea_all.bi_14),\n                                F.max(fea_all.bi_15),F.min(fea_all.bi_15),\n                                F.count(fea_all.query_id),\n                                F.avg(fea_all.title_len)),\n                            on='query_id').orderBy('index').rdd.map(lambda x:row(\n                                    x[\"index\"],\n                                    norm_fea(x,'w2v_eu_dist'),\n                                    norm_fea(x,'w2v_cos_dist'),\n                                    norm_fea(x,'edit_dist'),\n                                    norm_fea(x,'text_cos_sim'),\n                                    norm_fea(x,'text_jaccard_sim'),\n                                    norm_fea(x,'idf_cos_sim'),\n                                    norm_fea(x,'title_len'),\n                                    norm_fea(x,'word_count'),\n                                    norm_fea(x,'mean_pos'),\n                                    norm_fea(x,'word_ratio'),\n                                    norm_fea(x,'qt_min'),\n                                    norm_fea(x,'qt_mean'),\n                                    norm_fea(x,'tq_min'),\n                                    norm_fea(x,'tq_mean'),\n                                    norm_fea(x,'sum_hit'),\n                                    norm_fea(x,'mean_hit'),\n                                    norm_fea(x,'max_hit'),\n                                    norm_fea(x,'bi_1'),\n                                    norm_fea(x,'bi_2'),\n                                    norm_fea(x,'bi_3'),\n                                    norm_fea(x,'bi_4'),\n                                    norm_fea(x,'bi_5'),\n                                    norm_fea(x,'bi_6'),\n                                    norm_fea(x,'bi_7'),\n                                    norm_fea(x,'bi_8'),\n                                    norm_fea(x,'bi_9'),\n                                    norm_fea(x,'bi_10'),\n                                    norm_fea(x,'bi_11'),\n                                    norm_fea(x,'bi_12'),\n                                    norm_fea(x,'bi_13'),\n                                    norm_fea(x,'bi_14'),\n                                    norm_fea(x,'bi_15')\n                                    )).take(1)\n                                    # .toDF()\n                                    # .join(sdf_all.select('index','query_id','query_title_id','title','label'), on='index')","execution_count":65},{"metadata":{"id":"CB2935C1B5EB46B988E47838ED61C1BF","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"##","execution_count":67},{"metadata":{"id":"1AF57C382298468C9F1D9278D2D009B5","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def norm_fea(x,s):\n    max_str = 'max('+s+')'\n    min_str = 'min('+s+')'\n    return round(float(np.divide(x[s]-x[min_str]+ 1e-6,x[max_str]-x[min_str]+ 1e-6)),6)\n\nsdf_all =  fea_all.join(sdf_all.groupby('title').count().withColumnRenamed(\"count\", \"t_retri_count\"), on='title') \\\n    .join(fea_all.groupby('query_id').agg(\n            F.max(fea_all.w2v_eu_dist),F.min(fea_all.w2v_eu_dist),\n            F.max(fea_all.w2v_cos_dist),F.min(fea_all.w2v_cos_dist),\n            F.max(fea_all.edit_dist),F.min(fea_all.edit_dist),\n            F.max(fea_all.text_cos_sim),F.min(fea_all.text_cos_sim),\n            F.max(fea_all.text_jaccard_sim),F.min(fea_all.text_jaccard_sim),\n            F.max(fea_all.idf_cos_sim),F.min(fea_all.idf_cos_sim),\n            F.max(fea_all.title_len),F.min(fea_all.title_len),\n            F.max(fea_all.word_count),F.min(fea_all.word_count),\n            F.max(fea_all.mean_pos),F.min(fea_all.mean_pos),\n            F.max(fea_all.word_ratio),F.min(fea_all.word_ratio),\n            F.max(fea_all.qt_min),F.min(fea_all.qt_min),\n            F.max(fea_all.qt_mean),F.min(fea_all.qt_mean),\n            F.max(fea_all.tq_min),F.min(fea_all.tq_min),\n            F.max(fea_all.tq_mean),F.min(fea_all.tq_mean),\n            F.max(fea_all.sum_hit),F.min(fea_all.sum_hit),\n            F.max(fea_all.mean_hit),F.min(fea_all.mean_hit),\n            F.max(fea_all.max_hit),F.min(fea_all.max_hit),\n            F.max(fea_all.bi_1),F.min(fea_all.bi_1),\n            F.max(fea_all.bi_2),F.min(fea_all.bi_2),\n            F.max(fea_all.bi_3),F.min(fea_all.bi_3),\n            F.max(fea_all.bi_4),F.min(fea_all.bi_4),\n            F.max(fea_all.bi_5),F.min(fea_all.bi_5),\n            F.max(fea_all.bi_6),F.min(fea_all.bi_6),\n            F.max(fea_all.bi_7),F.min(fea_all.bi_7),\n            F.max(fea_all.bi_8),F.min(fea_all.bi_8),\n            F.max(fea_all.bi_9),F.min(fea_all.bi_9),\n            F.max(fea_all.bi_10),F.min(fea_all.bi_10),\n            F.max(fea_all.bi_11),F.min(fea_all.bi_11),\n            F.max(fea_all.bi_12),F.min(fea_all.bi_12),\n            F.max(fea_all.bi_13),F.min(fea_all.bi_13),\n            F.max(fea_all.bi_14),F.min(fea_all.bi_14),\n            F.max(fea_all.bi_15),F.min(fea_all.bi_15),\n            F.count(fea_all.query_id),\n            F.avg(fea_all.title_len)\n        ), on='query_id'\n    ).rdd.map(lambda x:row(\n            x[\"index\"],\n            norm_fea(x,'w2v_eu_dist'),\n            norm_fea(x,'w2v_cos_dist'),\n            norm_fea(x,'edit_dist'),\n            norm_fea(x,'text_cos_sim'),\n            norm_fea(x,'text_jaccard_sim'),\n            norm_fea(x,'idf_cos_sim'),\n            norm_fea(x,'title_len'),\n            norm_fea(x,'word_count'),\n            norm_fea(x,'mean_pos'),\n            norm_fea(x,'word_ratio'),\n            norm_fea(x,'qt_min'),\n            norm_fea(x,'qt_mean'),\n            norm_fea(x,'tq_min'),\n            norm_fea(x,'tq_mean'),\n            norm_fea(x,'sum_hit'),\n            norm_fea(x,'mean_hit'),\n            norm_fea(x,'max_hit'),\n            norm_fea(x,'bi_1'),\n            norm_fea(x,'bi_2'),\n            norm_fea(x,'bi_3'),\n            norm_fea(x,'bi_4'),\n            norm_fea(x,'bi_5'),\n            norm_fea(x,'bi_6'),\n            norm_fea(x,'bi_7'),\n            norm_fea(x,'bi_8'),\n            norm_fea(x,'bi_9'),\n            norm_fea(x,'bi_10'),\n            norm_fea(x,'bi_11'),\n            norm_fea(x,'bi_12'),\n            norm_fea(x,'bi_13'),\n            norm_fea(x,'bi_14'),\n            norm_fea(x,'bi_15')\n        )\n    ).toDF().join(sdf_all.select('index','query_id','query_title_id','title','label'), on='index').orderBy('index')\n                                        \n    \nsdf_test = fea_test.join(sdf_test.groupby('title').count().withColumnRenamed(\"count\", \"t_retri_count\"), on='title') \\\n    .join(fea_test.groupby('query_id').agg(\n            F.max(fea_test.w2v_eu_dist),F.min(fea_test.w2v_eu_dist),\n            F.max(fea_test.w2v_cos_dist),F.min(fea_test.w2v_cos_dist),\n            F.max(fea_test.edit_dist),F.min(fea_test.edit_dist),\n            F.max(fea_test.text_cos_sim),F.min(fea_test.text_cos_sim),\n            F.max(fea_test.text_jaccard_sim),F.min(fea_test.text_jaccard_sim),\n            F.max(fea_test.idf_cos_sim),F.min(fea_test.idf_cos_sim),\n            F.max(fea_test.title_len),F.min(fea_test.title_len),\n            F.max(fea_test.word_count),F.min(fea_test.word_count),\n            F.max(fea_test.mean_pos),F.min(fea_test.mean_pos),\n            F.max(fea_test.word_ratio),F.min(fea_test.word_ratio),\n            F.max(fea_test.qt_min),F.min(fea_test.qt_min),\n            F.max(fea_test.qt_mean),F.min(fea_test.qt_mean),\n            F.max(fea_test.tq_min),F.min(fea_test.tq_min),\n            F.max(fea_test.tq_mean),F.min(fea_test.tq_mean),\n            F.max(fea_test.sum_hit),F.min(fea_test.sum_hit),\n            F.max(fea_test.mean_hit),F.min(fea_test.mean_hit),\n            F.max(fea_test.max_hit),F.min(fea_test.max_hit),\n            F.max(fea_test.bi_1),F.min(fea_test.bi_1),\n            F.max(fea_test.bi_2),F.min(fea_test.bi_2),\n            F.max(fea_test.bi_3),F.min(fea_test.bi_3),\n            F.max(fea_test.bi_4),F.min(fea_test.bi_4),\n            F.max(fea_test.bi_5),F.min(fea_test.bi_5),\n            F.max(fea_test.bi_6),F.min(fea_test.bi_6),\n            F.max(fea_test.bi_7),F.min(fea_test.bi_7),\n            F.max(fea_test.bi_8),F.min(fea_test.bi_8),\n            F.max(fea_test.bi_9),F.min(fea_test.bi_9),\n            F.max(fea_test.bi_10),F.min(fea_test.bi_10),\n            F.max(fea_test.bi_11),F.min(fea_test.bi_11),\n            F.max(fea_test.bi_12),F.min(fea_test.bi_12),\n            F.max(fea_test.bi_13),F.min(fea_test.bi_13),\n            F.max(fea_test.bi_14),F.min(fea_test.bi_14),\n            F.max(fea_test.bi_15),F.min(fea_test.bi_15),\n            F.count(fea_test.query_id),\n            F.avg(fea_test.title_len)\n        ), on='query_id'\n    ).rdd.map(lambda x:row(\n            x[\"index\"],\n            norm_fea(x,'w2v_eu_dist'),\n            norm_fea(x,'w2v_cos_dist'),\n            norm_fea(x,'edit_dist'),\n            norm_fea(x,'text_cos_sim'),\n            norm_fea(x,'text_jaccard_sim'),\n            norm_fea(x,'idf_cos_sim'),\n            norm_fea(x,'title_len'),\n            norm_fea(x,'word_count'),\n            norm_fea(x,'mean_pos'),\n            norm_fea(x,'word_ratio'),\n            norm_fea(x,'qt_min'),\n            norm_fea(x,'qt_mean'),\n            norm_fea(x,'tq_min'),\n            norm_fea(x,'tq_mean'),\n            norm_fea(x,'sum_hit'),\n            norm_fea(x,'mean_hit'),\n            norm_fea(x,'max_hit'),\n            norm_fea(x,'bi_1'),\n            norm_fea(x,'bi_2'),\n            norm_fea(x,'bi_3'),\n            norm_fea(x,'bi_4'),\n            norm_fea(x,'bi_5'),\n            norm_fea(x,'bi_6'),\n            norm_fea(x,'bi_7'),\n            norm_fea(x,'bi_8'),\n            norm_fea(x,'bi_9'),\n            norm_fea(x,'bi_10'),\n            norm_fea(x,'bi_11'),\n            norm_fea(x,'bi_12'),\n            norm_fea(x,'bi_13'),\n            norm_fea(x,'bi_14'),\n            norm_fea(x,'bi_15')\n        )\n    ).toDF().join(sdf_test.select('index','query_id','query_title_id','title'), on='index').orderBy('index')\n                                        \n# sdf_all =  fea_all.join(sdf_all.groupby('title').count().withColumnRenamed(\"count\", \"t_retri_count\"), on='title') \\\n#     .join(fea_all.groupBy('query_id').agg({'query_id':'count','title_len':'avg'}), on='query_id') \\\n#     .orderBy('index')\n\n# sdf_test = fea_test.join(sdf_test.groupby('title').count().withColumnRenamed(\"count\", \"t_retri_count\"), on='title') \\\n#     .join(fea_test.groupBy('query_id').agg({'query_id':'count','title_len':'avg'}), on='query_id') \\\n#     .orderBy('index') \n\nprint(sdf_all.dtypes)\nprint(sdf_test.dtypes)                                        \n                                        ","execution_count":68},{"metadata":{"id":"FC626E1E6DDF4A1ABD03659FC6982EC8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"sdf_all.dtypes","execution_count":1},{"metadata":{"id":"D677DEB280DA481F927682963550E5D0","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"\n\n","execution_count":18},{"metadata":{"id":"12BA6E5644514D44AB1641C904CE93A8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":15},{"metadata":{"id":"29929869134A4D919DB970F84F8C0516","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":23},{"metadata":{"id":"DEBDAB9FE6B549A0A2ADF7E50BFE07D2","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# sdf_train = sdf_all\n\ns_len = all_count*0.9 + 9000000 \nsdf_dev = sdf_all.filter(sdf_all.index>=s_len)\nsdf_train = sdf_all.filter(sdf_all.index<s_len) # sdf_all.subtract(sdf_dev)\n","execution_count":25},{"metadata":{"id":"AF3507AA5F434D11997FDB21EBCAB433","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":26},{"metadata":{"id":"47260ED9A5F14747832E70D49EB2EB76","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# model training 100w 16min 22min  4.5h\nfrom pyspark.ml.classification import LinearSVC,GBTClassifier,LogisticRegression,LogisticRegressionModel,NaiveBayes, NaiveBayesModel\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline,PipelineModel\n\n \n#vector assembler\nvectorAssembler = VectorAssembler(\n    inputCols=[\n        \"w2v_eu_dist\",\"w2v_cos_dist\",\"edit_dist\",\n        \"text_cos_sim\",\"text_jaccard_sim\",#\"idf_cos_sim\",\n        \"title_len\",'word_count','mean_pos','word_ratio',\n        \"qt_min\",\"qt_mean\",\"tq_min\",\"tq_mean\",\n        't_retri_count','count(query_id)',\"avg(title_len)\"],\n    outputCol=\"assemblerFeatures\")\n\nlr = LogisticRegression(maxIter=10,regParam=0.0,elasticNetParam=0.0).setLabelCol(\"label\").setFeaturesCol(\"assemblerFeatures\")\nnb = NaiveBayes(smoothing=1.0).setLabelCol(\"label\").setFeaturesCol(\"assemblerFeatures\")\n# svm = LinearSVC(maxIter=5, regParam=0.01).setLabelCol(\"label\").setFeaturesCol(\"assemblerFeatures\")\ngbt = GBTClassifier(maxDepth=5,maxIter=1,maxMemoryInMB=8192,cacheNodeIds=True).setLabelCol(\"label\").setFeaturesCol(\"assemblerFeatures\") # 40min\nprint(\"model parameters:\\n\" + lr.explainParams())\n\npipeline =  Pipeline().setStages([vectorAssembler,lr])\nsdf_train.persist()\npipelineModel = pipeline.fit(sdf_train)\nsdf_train.unpersist()\n\n","execution_count":24},{"metadata":{"id":"644EECD442BE4E4CA1DEA610ECB54A2B","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# model save & load\npipelineModelPath = './models/svm_model_0629_2' \npipelineModel.save(pipelineModelPath)\n# pipelineModel = PipelineModel.load(pipelineModelPath)","execution_count":23},{"metadata":{"id":"36C99CF53B4B4A8F810754406DB653EA","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# result = pipelineModel.transform(sdf_test)\n# print(result.dtypes)  \n\n# result.select('rawPrediction','prediction').filter(col('query_id')<50250).take(10)","execution_count":26},{"metadata":{"id":"E74E59988F65474C8117A389FE135697","collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"code","outputs":[],"source":"# result generate 10min 23min  4h 6.5h\nresult_row = Row('query_id','query_title_id','prediction')\nresult = pipelineModel.transform(sdf_test) \\\n    .select('query_id','query_title_id','probability') \\\n    .rdd.map(lambda x:result_row(x[\"query_id\"],x[\"query_title_id\"],float(x[\"probability\"][1]))).toDF() \n    \nprint(result.dtypes)    ","execution_count":28},{"metadata":{"id":"E94D64C6D08745D59C82D057EDCA62C4","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# result save\nresult.toPandas().to_csv('./result_0629_demo.csv',header=False,index=False) ","execution_count":29},{"metadata":{"id":"2D8E0C480DD1477F86F6AF061F13F04A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# result lookup\n# pd.read_csv('./result_0625_7.csv',nrows=10).head()\nsdf_sub = spark.read.csv('./result_0629_demo.csv')\nprint(sdf_sub.dtypes)\nsdf_sub.show(2,truncate=True)\n","execution_count":30},{"metadata":{"id":"46D515428A704D5EB718A6AED63FC46F","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# result submit\n# 500w-> 0.5326 100w -> 0.5302\n# df729382a67a8015\n!./kesci_submit -token b321e54849cd528c -file ./result_0627_2.csv","execution_count":25},{"metadata":{"id":"AF68942DFD9D40609422641E3A600874","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":27},{"metadata":{"id":"28C13728A3164EBC88335F464AC30C8C","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"sdf_dev.dtypes\n# sdf_dev.show(1,truncate=True)","execution_count":33},{"metadata":{"id":"25997E592F6744239A5138451AC2251D","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# dev-result generate 5min\ndev_result_row = Row('query_id','label','prediction')\ndev_result = pipelineModel.transform(sdf_dev) \\\n    .select('query_id','label','probability') \\\n    .rdd.map(lambda x:dev_result_row(x[\"query_id\"],x[\"label\"],float(x[\"probability\"][1]))).toDF() \\\n\nprint(dev_result.dtypes)","execution_count":30},{"metadata":{"id":"C7ED327AD60747668D26CAECF7E8A8F7","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# dev-result valid  40s\nfrom sklearn.metrics import accuracy_score,roc_auc_score\n\ndef q_auc(df):\n    invalid_count = 0\n    query_ids = df['query_id'].unique()\n    aucs = []\n    for i,query_id in enumerate(query_ids):\n        current= df[df['query_id'] == query_id]\n        if sum(current['label']) not in [0, len(current['label'])]:\n            auc = roc_auc_score(current['label'], current['prediction'])  # 这里计算qauc\n        else:\n            auc = 0.5\n            invalid_count += 1\n        aucs.append(auc)\n    print(f'q_auc invalid_count = {invalid_count}')\n    return np.mean(aucs)\n    \nq_auc = q_auc(dev_result.toPandas())\n\nprint(q_auc) ","execution_count":31},{"metadata":{"id":"13BFE710632F4740B6A1B28B5CF02A2B","collapsed":false,"mdEditEnable":false},"cell_type":"markdown","source":"offline  | model   |   tip    \n-------- | -------- | -------- \n0.525 |   lr   |  100w,8-feat,0.9 \n0.527\t|\t\tlr\t|  drop('eu_dist')\n0.528 | lr  | word2vec-model-6kw-50-5, edit_dist\n0.529 | lr  | word2vec-model-6kw-50-5-2, edit_dist\n0.530 | lr  | word2vec-model-6kw-50-5-2, edit_dist, text_cos_sim\n0.529-0.537 | lr  | tfidf\n-0.539 | lr | tfidf+norm"},{"metadata":{"id":"51D3C0AC48F145F0A524CA4BF1577D80","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"5396E06F97AC4520890BA50B23A784BD","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def _map_to_pandas(rdds):\n    \"\"\" Needs to be here due to pickling issues \"\"\"\n    return [pd.DataFrame(list(rdds))]\ndef to_pandas(df, n_partitions=None):\n    \"\"\"\n    Returns the contents of `df` as a local `pandas.DataFrame` in a speedy fashion. The DataFrame is\n    repartitioned if `n_partitions` is passed.\n    :param df:              pyspark.sql.DataFrame\n    :param n_partitions:    int or None\n    :return:                pandas.DataFrame\n    \"\"\"\n    if n_partitions is not None: df = df.repartition(n_partitions)\n    \n    add_label = False  if 'label' not in df.columns else True\n    df_pand = df.rdd.mapPartitions(_map_to_pandas).collect()\n    for i,i_df in enumerate(df_pand):\n        try:\n            # print(i,i_df.dtypes,i_df.columns) #i=0:4\n            if add_label:\n                i_df.columns=['index','query_id','query_title_id','label','probability']\n            else:\n                i_df.columns=['index','query_id','query_title_id','probability']\n            i_df['prediction'] = [line[1] for line in i_df['probability']]\n            i_df.drop('probability', axis=1,inplace=True)\n            df_pand[i] = i_df\n        except:\n            continue\n    df_pand = pd.concat(df_pand)\n    df_pand.sort_values('index',ascending=True,inplace=True)\n    df_pand.drop(['index'],axis=1,inplace=True)\n    \n    if add_label:\n        df_pand.columns = ['query_id','query_title_id','label','prediction']\n    else:\n        df_pand.columns = ['query_id','query_title_id','prediction']\n    return df_pand","execution_count":3},{"metadata":{"id":"1A2B8EBEF1B84F4C81BAF4CB21E4B70A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":38},{"metadata":{"id":"FE1E214D815A4739AAB91F06288F8CF3","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"pd.read_csv(open('./tmp_result_100000.csv','r'),nrows=5,header=None)","execution_count":23},{"metadata":{"id":"97B52AA672914825A566F5525141F842","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# # 合并结果\n# import pandas as pd\n# header_names=['query_id','title_id','prediction']\n\n# file_paths = ['tmp_result_100000_0.csv',\n#               'tmp_result_100000_1.csv',\n#               'tmp_result_100000_2.csv',\n#               'tmp_result_100000_3.csv',\n#               'tmp_result_100000_4.csv']\n# def res_merge(res1,res2):\n#     result = pd.merge(res1, res2, on=['query_id', 'title_id'])\n#     result['prediction']=result['prediction_x']+result['prediction_y']\n#     result.drop(['prediction_x','prediction_y'],axis=1,inplace=True)\n#     return result\n\n# res_sum = pd.read_csv(open('./tmp_result_99999.csv','r'),nrows=None,header=None,names=header_names)\n# for path in file_paths:\n#     res_tmp = pd.read_csv(open(path,'r'),nrows=None,header=None,names=header_names)\n#     res_sum = res_merge(res_sum, res_tmp)  # 求和\n#     print(\"{} merge done.\".format(path))\n\n\n# res_sum['pred'] = res_sum['prediction']/(len(file_paths)+1)\n# res_sum.drop(['prediction'],axis=1,inplace=True)\n\n# res_sum","execution_count":39},{"metadata":{"id":"D1B1D9109BAF435BB9F25CBEA0F8653C","collapsed":true,"scrolled":false},"cell_type":"code","outputs":[],"source":"res_sum.to_csv('./result_0625_mean_400w.csv',header=None,index=0)\nprint(\"save done.\")\npd.read_csv(open('./result_0625_mean_400w.csv','r'),nrows=100,header=None)","execution_count":42},{"metadata":{"id":"4EBE9B1A44134EAB80F518D6CFE2AF61","collapsed":true,"scrolled":false},"cell_type":"code","outputs":[],"source":"pd.read_csv(open('./result_0625_1104.csv','r'),nrows=5,header=None)","execution_count":41},{"metadata":{"id":"27EE7B8B3F4845C3815A6CD3EBB94914"},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"ED0679042BDA4D20AE306936EAC0782B","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# result submit\n!./kesci_submit -token b321e54849cd528c -file result_0625_mean_400w.csv","execution_count":43},{"metadata":{"id":"64FB0B1F89A64E9BB1ACF0AE0CC4E6EB"},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}